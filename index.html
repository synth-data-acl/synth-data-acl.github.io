<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Synthetic Data in the Era of LLMs (tutorial at ACL 2025).">
  <meta name="keywords" content="Synthetic Data, ACL 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Synthetic Data in the Era of LLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  
  <style>
    .presenter-profiles {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 2rem;
      margin-top: 2rem;
    }
    
    .presenter-card {
      text-align: center;
      flex: 0 1 150px;
    }
    
    .presenter-image {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      object-fit: cover;
      margin-bottom: 0.5rem;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      transition: transform 0.3s ease;
    }
    
    .presenter-image:hover {
      transform: scale(1.05);
    }
    
    .presenter-name {
      font-size: 1.1rem;
      font-weight: 600;
      margin-bottom: 0.25rem;
      color: #03a5fc;
    }
    
    .presenter-affiliation {
      font-size: 1.2rem;
      color: #666;
    }
  </style>
</head>


<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Synthetic Data in the Era of LLMs</h1>
          <h1 class="title is-4 publication-title">Tutorial at <a href="https://2025.aclweb.org/program/">ACL 2025</a>
          <h1 class="title is-4 publication-title">Vienna, Austria</h1>

          <!-- Profile Pictures Section -->
          <div class="presenter-profiles">
            <div class="presenter-card">
              <img src="static/images/a.jpg" alt="Vijay Viswanathan" class="presenter-image">
              <a href="https://www.cs.cmu.edu/~vijayv"><div class="presenter-name">Vijay Viswanathan</div></a>
              <div class="presenter-affiliation">Carnegie Mellon University</div>
            </div>
            <div class="presenter-card">
              <img src="static/images/b.jpg" alt="Xiang Yue" class="presenter-image">
              <a href="https://xiangyue9607.github.io/"><div class="presenter-name">Xiang Yue</div></a>
              <div class="presenter-affiliation">Carnegie Mellon University</div>
            </div>
            <div class="presenter-card">
              <img src="static/images/c.jpg" alt="Alisa Liu" class="presenter-image">
              <a href="https://alisawuffles.github.io/"><div class="presenter-name">Alisa Liu</div></a>
              <div class="presenter-affiliation">University of Washington</div>
            </div>
            <div class="presenter-card">
              <img src="static/images/d.jpg" alt="Yizhong Wang" class="presenter-image">
              <a href="https://homes.cs.washington.edu/~yizhongw/"><div class="presenter-name">Yizhong Wang</div></a>
              <div class="presenter-affiliation">University of Washington</div>
            </div>
            <div class="presenter-card">
              <img src="static/images/e.jpg" alt="Graham Neubig" class="presenter-image">
              <a href="https://www.phontron.com/"><div class="presenter-name">Graham Neubig</div></a>
              <div class="presenter-affiliation">Carnegie Mellon University</div>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://us06web.zoom.us/rec/play/jLTcNKnh2lMcK63uNpW-9g-P_wUN85RFNLYGzxhL6gCkjqzPIGieJYXdeP_xGGuj6ZxP-nvYpDQQ22k9.9o2qq60z7LHyaMUq"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-camera"></i>
                  </span>
                  <span>Tutorial Recording</span>
                </a>
              </span>
              <span class="link-block">
                <a href="static/slides/slides.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>
             <span class="link-block">
                <a href="https://aclanthology.org/2025.acl-tutorials.7/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Presenter Biographies</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Progress in natural language processing has historically been driven by better data, and researchers today are increasingly using ‘synthetic data’ - data generated with the assistance of large language models - to make dataset construction faster and cheaper. 
          </p>
          <p>
            However, most synthetic data generation approaches are executed in an ad hoc manner and ‘reinvent the wheel’ rather than build on prior foundations. This tutorial seeks to build a shared understanding of recent progress in synthetic data generation from NLP and related fields by grouping and describing major methods, applications, and open problems.
          </p>
          <p>
            Our tutorial will be divided into four main sections. First, we will describe algorithms for producing high-quality synthetic data. Second, we will describe how synthetic data can be used to advance the general-purpose development and study of language models. Third, we will demonstrate how to customize synthetic data generation to support scenario-specific applications. Finally, we will discuss open questions about the production and use of synthetic data that must be answered to overcome some of their current limitations. Our goal is that by unifying recent advances in this emerging research direction, we can build foundations upon which the community can improve the rigor, understanding, and effectiveness of synthetic data moving forward.
            </p>
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

 <!-- Schedule. -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Schedule</h2>
        <div class="content has-text-justified">
          <p><strong>July 27, 2025 - Hall B</strong></p>
          <p><ul>
            <li><strong>2:00pm:</strong> How do we <strong>evaluate</strong> data quality? (15 minutes)</li>
            <li><strong>2:20pm:</strong> How do we <strong>create</strong> high-quality synthetic data? (35 minutes + Q&A)</li>
            <li><strong>3:05pm:</strong> How do we <strong>use</strong> synthetic data (Pt 1)? (25 minutes)</li>
            <li><strong>3:30pm:</strong> 30 minute break</li>
            <li><strong>4:00pm:</strong> How do we <strong>use</strong> synthetic data (Pt 2)? (20 minutes + Q&A)</li>
            <li><strong>4:25pm:</strong> Scenario-specific <strong>applications</strong> (35 minutes + Q&A)</li>
            <li><strong>5:00pm:</strong> Limitations and open questions (25 minutes + Q&A)</li>
            <li><strong>5:30pm:</strong> End</li>
          </ul></p>
        </div>
      </div>
    </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Bibliography</h2>
        <div class="content has-text-justified">
          <p>
    The papers referenced in the tutorial can be found below.
          </p>
            <p>
              <strong><a href="https://arxiv.org/abs/1503.02531" target="_blank">Distilling the Knowledge in a Neural Network</a></strong><br>
              Hinton et al., 2015
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/1511.06709" target="_blank">Improving Neural Machine Translation Models with Monolingual Data</a></strong><br>
              Sennrich et al., 2016
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/1606.07947" target="_blank">Sequence-Level Knowledge Distillation</a></strong><br>
              Kim & Rush, 2016
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/1811.07871" target="_blank">Scalable agent alignment via reward modeling: a research direction</a></strong><br>
              Leike et al., 2018
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2006.10413" target="_blank">Are Pretrained Language Models Symbolic Reasoners Over Knowledge?</a></strong><br>
              Kassner et al., 2020
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2004.10964" target="_blank">Don't Stop Pretraining: Adapt Language Models to Domains and Tasks</a></strong><br>
              Gururangan et al., 2020
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2104.07540" target="_blank">Generating Datasets with Pretrained Language Models</a></strong><br>
              Schick & Schütze, 2021
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2111.06467" target="_blank">SynthBio: A Case Study in Faster Curation of Text Datasets</a></strong><br>
              Yuan et al., 2021
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2212.08073" target="_blank">Constitutional AI: Harmlessness from AI Feedback</a></strong><br>
              Bai et al., 2022
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2202.03286" target="_blank">Red Teaming Language Models with Language Models</a></strong><br>
              Perez et al., 2022
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2212.10560" target="_blank">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></strong><br>
              Wang et al., 2022
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2203.14465" target="_blank">STaR: Bootstrapping Reasoning With Reasoning</a></strong><br>
              Zelikman et al., 2022
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2212.09689" target="_blank">Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor</a></strong><br>
              Honovich et al., 2022
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2201.05955" target="_blank">WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation</a></strong><br>
              Liu et al., 2022
            </p>
            <p>
              <strong><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank">Alpaca: A Strong, Replicable Instruction-Following Model</a></strong><br>
              Taori et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2304.08460" target="_blank">LongForm: Effective Instruction Tuning with Reverse Instructions</a></strong><br>
              Köksal et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2306.02707" target="_blank">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></strong><br>
              Mukherjee et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2310.08491" target="_blank">Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</a></strong><br>
              Kim et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2308.12261" target="_blank">Prompt2Model: Generating Deployable Models from Natural Language Instructions</a></strong><br>
              Viswanathan et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2304.06767" target="_blank">RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment</a></strong><br>
              Dong et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2308.06259" target="_blank">Self-Alignment with Instruction Backtranslation</a></strong><br>
              Li et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2303.17651" target="_blank">Self-Refine: Iterative Refinement with Self-Feedback</a></strong><br>
              Madaan et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2212.10465" target="_blank">SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization</a></strong><br>
              Kim et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2305.15717" target="_blank">The False Promise of Imitating Proprietary LLMs</a></strong><br>
              Gudibande et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2306.11644" target="_blank">Textbooks Are All You Need</a></strong><br>
              Gunasekar et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2310.01377" target="_blank">UltraFeedback: Boosting Language Models with Scaled AI Feedback</a></strong><br>
              Cui et al., 2023
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2304.12244" target="_blank">WizardLM: Empowering Large Pre-trained Language Models to Follow Complex Instructions</a></strong><br>
              Xu et al., 2023
            </p>
            <p>
              <strong><a href="https://www.nature.com/articles/s41586-024-07566-y" target="_blank">AI models collapse when trained on recursively generated data</a></strong><br>
              Shumailov et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2404.14361" target="_blank">Better Synthetic Data by Retrieving and Transforming Existing Datasets</a></strong><br>
              Gandhi et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2502.19249" target="_blank">Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases</a></strong><br>
              Hu et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2507.18624" target="_blank">Checklists Are Better Than Reward Models For Aligning Language Models</a></strong><br>
              Viswanathan et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2412.03679" target="_blank">Evaluating Language Models as Synthetic Data Generators</a></strong><br>
              Kim et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2403.13787" target="_blank">Evaluating Reward Models for Language Modeling</a></strong><br>
              Lambert et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2410.19133" target="_blank">Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback</a></strong><br>
              Miranda et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2405.03548" target="_blank">MAmmoTH2: Scaling Instructions from the Web</a></strong><br>
              Yue et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2401.16380" target="_blank">Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling</a></strong><br>
              Maini et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2407.12874" target="_blank">SELF-GUIDE: Better Task-Specific Instruction Following via Self-Synthetic Finetuning</a></strong><br>
              Zhao et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2409.07431" target="_blank">Synthetic continued pretraining</a></strong><br>
              Yang et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2409.02098" target="_blank">Synthetic Dataset Generation Through Corpus Retrieval and Augmentation</a></strong><br>
              Ziegler et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2410.06554" target="_blank">The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models</a></strong><br>
              Chen et al., 2024
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2503.01067" target="_blank">All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning</a></strong><br>
              Swamy et al., 2025
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2501.12948" target="_blank">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL</a></strong><br>
              Deepseek-AI, 2025
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2505.20161" target="_blank">Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning</a></strong><br>
              Jung et al., 2025
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2507.07229" target="_blank">SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains</a></strong><br>
              Ramesh et al., 2025
            </p>
            <p>
              <strong><a href="https://arxiv.org/abs/2503.15477" target="_blank">What Makes a Reward Model a Good Teacher? An Optimization Perspective</a></strong><br>
              Razin et al., 2025
            </p>

        </div>
      </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{synth-data-tutorial,
    title = "Synthetic Data in the Era of Large Language Models",
    author = "Viswanathan, Vijay  and
      Yue, Xiang  and
      Liu, Alisa  and
      Wang, Yizhong  and
      Neubig, Graham",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 5: Tutorial Abstracts)",
    publisher = "Association for Computational Linguistics",
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            
            This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
